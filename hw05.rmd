---
title: "Homework 05"
author: "Anika Cartas"
date: "Wednesday, April 08, 2015"
output: word_document
---

###0. Read Data

The table lists measurements of DC output at different wind velocity.

* Dependent variable: DC
* Independent variable: Wind Velocity

```{r}
data <- data.frame(
  vel = c(5,6,3.4,2.7,10,9.7,9.55,3.05,8.15,6.2,2.9,6.35,4.6,5.8,7.4,3.6,7.85,8.8,7,5.45,9.1,10.2,4.1,3.95,2.45),
  dc = c(1.582,1.822,1.057,0.5,2.236,2.386,2.294,0.558,2.166,1.866,0.653,1.93,1.562,1.737,2.088,1.137,2.179,2.112,1.8,1.501,2.303,2.310,1.194,1.144,0.123))

data <- data[with(data,order(vel)),]

attach(data)
```

###1. Linear Regression

```{r}
reg.linear <- lm(dc~vel)

plot(dc~vel, main="Linear Regression")
abline(reg.linear, col="red")
```

###2. Polynomial Regression

```{r}
plot(dc~vel, main="Polynomial Regression")
lines(lowess(dc~vel), col="green")
```

Using lowess, we visualize this to be a 4th degree polynomial. However, let's see if we can get more details.

```{r}
fit2 <- lm(dc ~ poly(vel, 2, raw=TRUE))
fit3 <- lm(dc ~ poly(vel, 3, raw=TRUE))
fit4 <- lm(dc ~ poly(vel, 4, raw=TRUE))
fit5 <- lm(dc ~ poly(vel, 5, raw=TRUE))
summary(fit2)
summary(fit3)
summary(fit4)
summary(fit5)
```

Depending on how specific we want to get, it looks like we are adding significant variables up to 4th degree. I am using the significance levels of each added degree of the variable to determine my cut-off point, because due to overfitting errors the R^2 and RMSE value are not good indicators of which model is best.

```{r}
plot(dc~vel, main="Polynomial Regression")
lines(lowess(dc~vel), col="green")
reg.poly <- lm(dc ~ poly(vel, 4, raw=TRUE))
pred.poly <- predict(reg.poly)
lines(vel, pred.poly,col="darkgreen") 
legend("bottomright", c("Estimate","Modelled"), col=c("green","darkgreen"), lwd=2)
```

###3. Transformation Based Regression

The Box-Cox Power Transformation identifies an appropriate exponent (lambda) to transform the data into a normal shape. The Lambda value indicates the power to which all data should be raised. 

When lambda is plotted against log likelihood, the best lambda is the one that maximizes the log likelihood.

I am doing this two ways. One in which I use the boxcox method to determine a transformation for the dependent variable, and one in which I transform the independent variable. I will compare their results at the end in the discussion section.

####3.1. Transformation of Independent Variable

```{r}
library(MASS)
bc.ind <-boxcox(vel ~ dc)

(maxll <- max(bc.ind$y))
(lambda <- bc.ind$x[which(bc.ind$y == maxll)])
```

Maximum log likelihood of 27.17 is achieved when lambda is equal to -0.75.

```{r}
reg.ind<-lm(dc ~ I(vel^lambda))
plot(dc ~ I(vel^lambda), main="Transformation of Independent Variable")
abline(reg.ind, col="blue")
```

####3.2. Transformation of Dependent Variable

```{r}
bc.dep <- boxcox(dc ~ vel)
(maxll.dep <- max(bc.dep$y))
(lambda.dep <- bc.dep$x[which(bc.dep$y == maxll.dep)])
```

Maximum log likelihood of 17.65 is achieved when lambda is equal to 2.

```{r}
reg.dep<-lm(I(dc^lambda.dep) ~ vel)
plot(I(dc^lambda.dep) ~ vel, main="Transformation of Dependent Variable")
abline(reg.dep, col="red")
```

###4. Nonlinear Regression

Following advice from Simran, I will investigate the Gompertz Growth Model.

The nls model will allow me to find good values for the parameters. One important thing to note that I was unaware of is that I do not need to supply values for the parameters in the SS... functions, just variable names.

```{r}
reg.gomp <- nls(dc ~ SSgompertz(vel, Asym, b2, b3))
pred.gomp <- predict(reg.gomp)

plot(dc ~ vel, main="Nonlinear Regression (Gompertz)")
lines(vel,pred.gomp,col="red")

```

I will also try the Logistic Model.

```{r}
reg.log <- nls(dc ~ SSlogis(vel, Asym, xmid, scal))
pred.log <- predict(reg.log)

plot(dc ~ vel, main="Nonlinear Regression (Logistic)")
lines(vel,pred.log,col="blue")
```

###5. Discussion

To determine which model is best, I will calculate the RMSE of observed vs predicted values.

```{r}
pred.linear <- predict(reg.linear)
rmse.linear <- sqrt(mean((dc-pred.linear)^2))
plot(pred.linear~dc, col="blue",main=sprintf("Linear Regression (RMSE %.03f)",rmse.linear),xlab="Observed",ylab="Linear Prediction")
lines(dc, dc, col="red")
```

```{r}
rmse.poly <- sqrt(mean((dc-pred.poly)^2))
plot(pred.poly~dc, col="darkgreen",main=sprintf("Polynomial Regression (RMSE %.03f)",rmse.poly),xlab="Observed",ylab="Polynomial Prediction")
lines(dc, dc, col="red")
```

```{r}
pred.ind <- predict(reg.ind)
rmse.ind <- sqrt(mean((dc-pred.ind)^2))
plot(pred.ind ~ dc, col="blue",main=sprintf("Transf. of Ind. (RMSE %.03f)",rmse.ind),xlab="Observed",ylab="Predicted")
lines(dc,dc,col="red")
```

```{r}
pred.dep <- predict(reg.dep)
rmse.dep <- sqrt(mean((I(dc^lambda.dep)-pred.dep)^2))
plot(pred.dep ~ I(dc^lambda.dep), col="darkgreen",main=sprintf("Transf. of Dep. (RMSE %.03f)",rmse.dep),xlab="Observed",ylab="Predicted")
lines(I(dc^lambda.dep),I(dc^lambda.dep),col="red")
```

```{r}
rmse.gomp <- sqrt(mean((dc-pred.gomp)^2))
plot(pred.gomp ~ dc, col="blue",main=sprintf("Nonlinear - Gompertz (RMSE %.03f)",rmse.gomp),xlab="Observed",ylab="Predicted")
lines(dc,dc,col="red")
```

```{r}
rmse.log <- sqrt(mean((dc-pred.log)^2))
plot(pred.log ~ dc, col="darkgreen",main=sprintf("Nonlinear - Logistic (RMSE %.03f)",rmse.log),xlab="Observed",ylab="Predicted")
lines(dc,dc,col="red")
```

```{r}
(sort(c("Linear"=rmse.linear, "Polynomial"=rmse.poly,"Transf. DC"=rmse.dep, "Transf. Vel"=rmse.ind, "Gompertz"= rmse.gomp, "Logistic"=rmse.log)))
```
Using RMSE as our analysis point, we could determine that the polynomial regression is our best model. However, since polynomial regressions may have a problem of overfitting, we should be wary of using this model to infer anything about unknown data, especially since we know the 4th degree polynomial will dip back down instead of continuing to increase, which is what we would expect given our knowledge about our dataset.

Since the transformation of the Independent Variable is pretty close, I would choose this model.
